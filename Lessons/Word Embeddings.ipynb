{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network: Skip Gram\n",
    "\n",
    "#### The Fake Task\n",
    "\n",
    "We’re going to build the neural network to perform a  “fake” task, which indirectly gives us those word vectors that we are really after.\n",
    "\n",
    "We’re going to train the neural network to do the following. Given a specific word in the middle of a sentence (the input word), look at the words nearby and pick one at random. The network is going to tell us the probability for every word in our vocabulary of being the “nearby word” that we chose.\n",
    "By \"nearby\", we mean there is actually a \"window size\" parameter to the algorithm. A typical window size might be 5, meaning 5 words behind and 5 words ahead (10 in total).\n",
    "\n",
    "The output probabilities are going to relate to how likely it is find each vocabulary word nearby our input word. For example, if you gave the trained network the input word “Soviet”, the output probabilities are going to be much higher for words like “Union” and “Russia” than for unrelated words like “watermelon” and “kangaroo”.\n",
    "\n",
    "We’ll train the neural network to do this by feeding it word pairs found in our training documents. The below example shows some of the training samples (word pairs) we would take from the sentence “The quick brown fox jumps over the lazy dog.” The word highlighted in blue is the input word.\n",
    "\n",
    "<img src=\"Figures/training_data_word2vec.png\" width=\"75%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture \n",
    "\n",
    "We’re going to represent an input word like “ants” as a one-hot vector. This vector will have 10,000 components (one for every word in our vocabulary) and we’ll place a “1” in the position corresponding to the word “ants”, and 0s in all of the other positions.\n",
    "\n",
    "The output of the network is a single vector (also with 10,000 components) containing, for every word in our vocabulary, the probability that a randomly selected nearby word is that vocabulary word.\n",
    "\n",
    "Here’s the architecture of our neural network.\n",
    "\n",
    "<img src=\"Figures/skip_gram_net_arch.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Hidden Layer\n",
    "\n",
    "For our example, we’re going to say that we’re learning word vectors with 300 features. So the hidden layer is going to be represented by a weight matrix with 10,000 rows (one for every word in our vocabulary) and 300 columns (one for every hidden neuron).\n",
    "\n",
    "*300 features is what Google used in their published model trained on the Google news dataset (you can download it from [here](https://code.google.com/archive/p/word2vec/)). The number of features is a \"hyper parameter\" that you would just have to tune to your application (that is, try different values and see what yields the best results).*\n",
    "\n",
    "If you look at the rows of this weight matrix, these are actually what will be our word vectors!\n",
    "\n",
    "<img src=\"Figures/word2vec_weight_matrix_lookup_table.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Effect of matrix multiplication with a one-hot vector\n",
    "\n",
    "That one-hot vector is almost all zeros… If you multiply a 1 x 10,000 one-hot vector by a 10,000 x 300 matrix, it will effectively just select the matrix row corresponding to the “1”. Here’s a small example to give you a visual.\n",
    "\n",
    "<img src=\"Figures/matrix_mult_w_one_hot.png\" width=\"70%\">\n",
    "\n",
    "This means that the hidden layer of this model is really just operating as a lookup table. The output of the hidden layer is just the “word vector” for the input word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Output Layer\n",
    "\n",
    "The 1 x 300 word vector for “ants” then gets fed to the output layer. The output layer is a softmax regression classifier. There’s an in-depth tutorial on Softmax Regression here, but the gist of it is that each output neuron (one per word in our vocabulary!) will produce an output between 0 and 1, and the sum of all these output values will add up to 1.\n",
    "\n",
    "Specifically, each output neuron has a weight vector which it multiplies against the word vector from the hidden layer, then it applies the function exp(x) to the result. Finally, in order to get the outputs to sum up to 1, we divide this result by the sum of the results from all 10,000 output nodes.\n",
    "\n",
    "Here’s an illustration of calculating the output of the output neuron for the word “car”.\n",
    "\n",
    "<img src=\"Figures/output_weights_function.png\"  width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intuition\n",
    "\n",
    "If two different words have very similar “contexts” (that is, what words are likely to appear around them), then our model needs to output very similar results for these two words. And one way for the network to output similar context predictions for these two words is if the word vectors are similar. So, **if two words have similar contexts, then our network is motivated to learn similar word vectors for these two words!** \n",
    "\n",
    "And what does it mean for two words to have similar contexts? I think you could expect that **synonyms** like “intelligent” and “smart” would have very similar contexts. Or that words that are related, like “engine” and “transmission”, would probably have similar contexts as well.\n",
    "\n",
    "This can also handle stemming for you – the network will likely learn similar word vectors for the words “ant” and “ants” because these should have similar contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Another View as autoencoder\n",
    "\n",
    "We have seen autoencoders before.\n",
    "\n",
    "<img src=\"Figures/word2vec-skip-gram.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CBOW\n",
    "\n",
    "Unlike a language model that can only base its predictions on past words, as it is assessed based on its ability to predict each next word in the corpus, a model that only aims to produce accurate word embeddings is not subject to such restriction. Mikolov et al. therefore use both the n words before and after the target word to predict it as shown in the Figure. This is known as a **continuous bag of words** (CBOW), owing to the fact that it uses continuous representations whose order is of no importance.\n",
    "\n",
    "<img src=\"Figures/word2vec-cbow.png\" width=\"80%\" >\n",
    "\n",
    "According to the authors Mikolov et al, here is the difference between Skip-gram and CBOW:\n",
    "\n",
    "- Skip-gram: works well with small amount of the training data, represents well even rare words or phrases\n",
    "\n",
    "- CBOW: several times faster to train than the skip-gram, slightly better accuracy for the frequent words\n",
    "\n",
    "Sometimes the different architectures are depicted by this graph:\n",
    "\n",
    "<img src=\"Figures/CBOW-vs-SkipGram.png\" width=\"85%\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GloVe\n",
    "\n",
    "Original paper: GloVe: Global Vectors for Word Representation https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "Count-based model - GloVe is essentially a log-bilinear model with a weighted least-squares objective The model rests on a rather simple idea that ratios of word-word co-occurrence probabilities have the potential for encoding some form of meaning which can be encoded as vector differences Therefore, the training objective is to learn word vectors such that their dot product equals the logarithm of the words’ probability of co-occurrence. Loss function:\n",
    "\n",
    "$$\n",
    "J = \\sum_{i, j=1}^V f(X_{ij}) \\: (w_i^T \\tilde{w}_j + b_i + \\tilde{b}_j  - \\text{log} \\: X_{ij})^2\n",
    "$$\n",
    "\n",
    "where $w_i$ and $b_i$ are the word vector and bias respectively of word $i$, $\\tilde{w}_j$ and $b_j$ are the context word vector and bias respectively of word $j$, $X_{ij}$ is the number of times word $i$ occurs in the context of word $j$, and $f$ is a weighting function that assigns relatively lower weight to rare and frequent co-occurrences.\n",
    "\n",
    "However, when we control for all the training hyper-parameters, the embeddings generated using the two methods tend to perform very similarly in downstream NLP tasks. The additional benefits of GloVe over word2vec is that it is easier to parallelize the implementation which means it's easier to train over more data, which, with these models, is always A Good Thing.\n",
    "\n",
    "- In word2vec, Skipgram models try to capture co-occurrence one window at a time\n",
    "- Glove tries to capture the global counts of overall statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word2vec in spacy\n",
    "\n",
    "- *spaCy* can compare two objects and predict similarity\n",
    "- *Doc.similarity()*, *Span.similarity()* and *Token.similarity()*\n",
    "- Take another object and return a similarity score ( 0 to 1 )\n",
    "- Important: needs a model that has word vectors included, for example:\n",
    "    * *en_core_web_md* (medium model)\n",
    "    * *en_core_web_lg* (large model)\n",
    "    * NOT *en_core_web_sm* (small model)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Similarity is determined using word vectors\n",
    "- Multi-dimensional meaning representations of words\n",
    "- Generated using an algorithm like and lots of text\n",
    "- Can be added to spaCy's statistical models\n",
    "- Default: cosine similarity, but can be adjusted\n",
    "- Doc and Span vectors default to average of token vectors\n",
    "- Short phrases are better than long documents with many irrelevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.73695457\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199091710787739\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.4095    -0.22693    0.25362   -0.36055   -0.37095   -0.35181\n",
      "  0.50669   -0.77897   -0.32571    1.4895     0.052438  -0.36751\n",
      " -0.074025   0.37078    0.063077   0.32274    0.346      0.64214\n",
      " -0.09583    0.14303   -0.33826    0.79005   -0.7136    -0.050134\n",
      " -0.46467   -0.067917  -0.32107    0.042919   0.018576   0.59272\n",
      " -0.032392   0.72779    0.26002    0.30401    0.43033    0.25546\n",
      " -0.37986   -0.14398   -0.54399   -0.46181    0.11046   -0.034391\n",
      " -0.10458   -0.069689   0.091839  -0.19097   -0.057108   0.61218\n",
      " -0.19544   -0.31698   -0.46372    0.088749  -0.052501  -0.27969\n",
      "  0.025125  -0.42097   -0.069404  -0.038672  -0.26489    0.10911\n",
      " -0.084848  -0.23826    0.61538    0.0039223  0.20285    0.56085\n",
      "  0.015419   0.30707    0.19435   -0.20358   -0.18724   -0.10311\n",
      " -0.46468   -0.16804    0.22614    0.040657  -0.5147     0.46701\n",
      "  0.61985   -0.46281   -0.8657     0.26458   -0.015476   0.12292\n",
      "  0.084031  -0.07936    0.58967   -0.011092  -0.3795    -0.053612\n",
      "  0.21134    0.46996    0.11207   -0.16461    0.43344   -0.40563\n",
      " -0.056622   0.37558   -0.41568    0.48919   -0.21899    0.070619\n",
      " -0.17975   -0.31542    0.030309  -0.50396   -0.3454     0.046303\n",
      "  0.65506   -0.34406    0.26439    0.65116    0.064871  -0.21301\n",
      " -0.33609   -0.13405    0.021996   0.13674    0.39605   -0.53568\n",
      " -0.4572     0.015033   0.34131    0.081045   0.13898    0.077339\n",
      "  0.12537   -0.36673   -0.45194    0.18747   -0.80829   -0.72773\n",
      " -0.16877   -0.10304    0.61342   -0.11578   -0.08559    0.32745\n",
      "  0.20161    0.16451   -2.6898    -0.13789    0.38222   -0.14568\n",
      " -0.54351   -0.057114   0.052728   0.21628    0.17275   -0.52548\n",
      "  0.33424   -0.11197    0.32572    0.073191   0.55281   -0.42832\n",
      "  0.66409   -0.88453   -0.097977   0.44619   -0.32691   -0.24954\n",
      " -0.077847  -0.0092577  0.52026   -0.22013    0.097384   0.55009\n",
      " -0.17778    0.31343   -0.27513   -0.040442   0.21805    0.17002\n",
      " -0.072219   0.0059726  0.35768    0.084917  -0.86797    0.35865\n",
      "  0.065546   0.10703    0.2024    -0.38667   -0.054235   0.2198\n",
      " -0.49117   -0.018379  -0.10444    0.55107   -0.49634   -0.29664\n",
      "  0.33894   -0.0087991 -0.03863   -0.54128    0.0079764  0.054724\n",
      " -0.2015    -0.18551    0.13176    0.27923    0.076447  -0.048008\n",
      " -0.055424  -0.34943    0.39729    0.2457     0.029499  -0.68098\n",
      " -0.6871     0.59309    0.074554   0.20225   -0.036478   0.14827\n",
      " -0.25059    0.26117   -0.31805    0.45437   -0.038272   0.028742\n",
      "  0.27339   -0.33354    0.91142    0.31166    0.11014    0.042104\n",
      " -0.12473    0.51306    0.16476    0.072608   0.31229   -0.10835\n",
      "  0.21623   -0.63722    0.21012   -0.083516  -0.17219   -0.51574\n",
      "  0.12891   -0.42211    0.031661  -0.27926    0.32035   -0.76117\n",
      "  0.32241    0.14613   -0.23227   -0.8071     0.11194    0.38073\n",
      " -0.022643  -0.24833   -0.26833    0.4066    -0.13731   -0.3489\n",
      " -0.6394     0.21267   -0.099964   0.37609   -0.23807   -0.4519\n",
      " -0.29246   -0.06527    0.53402    0.49704   -1.2046     0.028652\n",
      " -0.76803   -0.14139   -0.27493    0.090442   0.1094    -0.11903\n",
      "  0.26246   -0.094261  -0.004892  -0.22072    0.98163    0.68397\n",
      " -0.046815  -0.26183   -0.35914    0.10693    0.047385  -0.12417\n",
      " -0.17449    0.056819   0.086431   0.28052   -0.044373  -0.29452\n",
      " -0.19942    0.59475   -0.33682   -0.34563    0.12043   -0.36494  ]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"king equal queen minus woman plus man ?\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[2].vector)\n",
    "#print(doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tasks: \n",
    "- compare with word2vec\n",
    "- find function that computes the closest neighbor\n",
    "- check the \"arithmetic\" relations such as \"king - man + woman is queen\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity depends on the application context\n",
    "\n",
    "- Useful for many applications: recommendation systems, flagging duplicates etc.\n",
    "- There's no objective definition of \"similarity\"\n",
    "- Depends on the context and what application needs to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9501446702124066\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(\"I like cats\")\n",
    "doc2 = nlp(\"I hate cats\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note2Self:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
